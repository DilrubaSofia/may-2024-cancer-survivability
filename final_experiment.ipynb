{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 991 entries, 0 to 990\n",
      "Data columns (total 12 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   ajcc_pathologic_stage  991 non-null    int64  \n",
      " 1   primary_diagnosis      991 non-null    object \n",
      " 2   prior_malignancy       991 non-null    object \n",
      " 3   year_of_diagnosis      991 non-null    float64\n",
      " 4   prior_treatment        991 non-null    object \n",
      " 5   pharm_treatment        991 non-null    int64  \n",
      " 6   radiation              991 non-null    int64  \n",
      " 7   ethnicity              991 non-null    object \n",
      " 8   race                   991 non-null    object \n",
      " 9   vital_status           991 non-null    object \n",
      " 10  age_at_index           991 non-null    int64  \n",
      " 11  year_of_birth          991 non-null    float64\n",
      "dtypes: float64(2), int64(4), object(6)\n",
      "memory usage: 93.0+ KB\n"
     ]
    }
   ],
   "source": [
    "readdata = pd.read_csv(\"clinical_data.csv\").drop(columns=['case_id'])\n",
    "\n",
    "readdata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "     ajcc_pathologic_stage                 primary_diagnosis prior_malignancy  \\\n0                        1  Infiltrating duct carcinoma, NOS               no   \n1                        3  Infiltrating duct carcinoma, NOS               no   \n2                        1            Lobular carcinoma, NOS              yes   \n3                        5            Lobular carcinoma, NOS              yes   \n4                        8  Infiltrating duct carcinoma, NOS               no   \n..                     ...                               ...              ...   \n986                      1  Infiltrating duct carcinoma, NOS               no   \n987                      8  Infiltrating duct carcinoma, NOS               no   \n988                      1  Infiltrating duct carcinoma, NOS               no   \n989                      3            Lobular carcinoma, NOS               no   \n990                      4  Infiltrating duct carcinoma, NOS               no   \n\n     year_of_diagnosis prior_treatment  pharm_treatment  radiation  \\\n0               2010.0              No                1          0   \n1               2010.0              No                0          0   \n2               2012.0              No                1          0   \n3               2010.0              No                1          1   \n4               2009.0              No                1          0   \n..                 ...             ...              ...        ...   \n986             2003.0              No                1          1   \n987             2013.0              No                0          0   \n988             2009.0              No                1          0   \n989             2011.0              No                1          1   \n990             2010.0              No                1          0   \n\n                  ethnicity                       race vital_status  \\\n0    not hispanic or latino                      white        Alive   \n1    not hispanic or latino                      white        Alive   \n2    not hispanic or latino                      white        Alive   \n3    not hispanic or latino                      white        Alive   \n4              not reported               not reported        Alive   \n..                      ...                        ...          ...   \n986  not hispanic or latino                      white        Alive   \n987  not hispanic or latino  black or african american        Alive   \n988  not hispanic or latino  black or african american        Alive   \n989  not hispanic or latino                      white        Alive   \n990  not hispanic or latino                      asian        Alive   \n\n     age_at_index  year_of_birth  \n0              60         1950.0  \n1              56         1954.0  \n2              61         1951.0  \n3              71         1939.0  \n4              76         1933.0  \n..            ...            ...  \n986            60         1943.0  \n987            35         1978.0  \n988            46         1963.0  \n989            71         1940.0  \n990            58         1952.0  \n\n[991 rows x 12 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ajcc_pathologic_stage</th>\n      <th>primary_diagnosis</th>\n      <th>prior_malignancy</th>\n      <th>year_of_diagnosis</th>\n      <th>prior_treatment</th>\n      <th>pharm_treatment</th>\n      <th>radiation</th>\n      <th>ethnicity</th>\n      <th>race</th>\n      <th>vital_status</th>\n      <th>age_at_index</th>\n      <th>year_of_birth</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Infiltrating duct carcinoma, NOS</td>\n      <td>no</td>\n      <td>2010.0</td>\n      <td>No</td>\n      <td>1</td>\n      <td>0</td>\n      <td>not hispanic or latino</td>\n      <td>white</td>\n      <td>Alive</td>\n      <td>60</td>\n      <td>1950.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>Infiltrating duct carcinoma, NOS</td>\n      <td>no</td>\n      <td>2010.0</td>\n      <td>No</td>\n      <td>0</td>\n      <td>0</td>\n      <td>not hispanic or latino</td>\n      <td>white</td>\n      <td>Alive</td>\n      <td>56</td>\n      <td>1954.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>Lobular carcinoma, NOS</td>\n      <td>yes</td>\n      <td>2012.0</td>\n      <td>No</td>\n      <td>1</td>\n      <td>0</td>\n      <td>not hispanic or latino</td>\n      <td>white</td>\n      <td>Alive</td>\n      <td>61</td>\n      <td>1951.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n      <td>Lobular carcinoma, NOS</td>\n      <td>yes</td>\n      <td>2010.0</td>\n      <td>No</td>\n      <td>1</td>\n      <td>1</td>\n      <td>not hispanic or latino</td>\n      <td>white</td>\n      <td>Alive</td>\n      <td>71</td>\n      <td>1939.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8</td>\n      <td>Infiltrating duct carcinoma, NOS</td>\n      <td>no</td>\n      <td>2009.0</td>\n      <td>No</td>\n      <td>1</td>\n      <td>0</td>\n      <td>not reported</td>\n      <td>not reported</td>\n      <td>Alive</td>\n      <td>76</td>\n      <td>1933.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>986</th>\n      <td>1</td>\n      <td>Infiltrating duct carcinoma, NOS</td>\n      <td>no</td>\n      <td>2003.0</td>\n      <td>No</td>\n      <td>1</td>\n      <td>1</td>\n      <td>not hispanic or latino</td>\n      <td>white</td>\n      <td>Alive</td>\n      <td>60</td>\n      <td>1943.0</td>\n    </tr>\n    <tr>\n      <th>987</th>\n      <td>8</td>\n      <td>Infiltrating duct carcinoma, NOS</td>\n      <td>no</td>\n      <td>2013.0</td>\n      <td>No</td>\n      <td>0</td>\n      <td>0</td>\n      <td>not hispanic or latino</td>\n      <td>black or african american</td>\n      <td>Alive</td>\n      <td>35</td>\n      <td>1978.0</td>\n    </tr>\n    <tr>\n      <th>988</th>\n      <td>1</td>\n      <td>Infiltrating duct carcinoma, NOS</td>\n      <td>no</td>\n      <td>2009.0</td>\n      <td>No</td>\n      <td>1</td>\n      <td>0</td>\n      <td>not hispanic or latino</td>\n      <td>black or african american</td>\n      <td>Alive</td>\n      <td>46</td>\n      <td>1963.0</td>\n    </tr>\n    <tr>\n      <th>989</th>\n      <td>3</td>\n      <td>Lobular carcinoma, NOS</td>\n      <td>no</td>\n      <td>2011.0</td>\n      <td>No</td>\n      <td>1</td>\n      <td>1</td>\n      <td>not hispanic or latino</td>\n      <td>white</td>\n      <td>Alive</td>\n      <td>71</td>\n      <td>1940.0</td>\n    </tr>\n    <tr>\n      <th>990</th>\n      <td>4</td>\n      <td>Infiltrating duct carcinoma, NOS</td>\n      <td>no</td>\n      <td>2010.0</td>\n      <td>No</td>\n      <td>1</td>\n      <td>0</td>\n      <td>not hispanic or latino</td>\n      <td>asian</td>\n      <td>Alive</td>\n      <td>58</td>\n      <td>1952.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>991 rows Ã— 12 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 600x500 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 600x500 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 600x500 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 600x500 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 600x500 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_variable = 'vital_status'\n",
    "\n",
    "# Function to visualize categorical features\n",
    "def visualize_categorical_features(df, target_variable):\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    categorical_columns = [col for col in categorical_columns if col != target_variable]\n",
    "    for column in categorical_columns:\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        sns.countplot(x=column, hue=target_variable, data=df)\n",
    "        plt.title(f'Relationship between {column} and {target_variable}')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{column}.png\", bbox_inches='tight')\n",
    "        plt.clf()\n",
    "\n",
    "# Visualize categorical features\n",
    "visualize_categorical_features(readdata, target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x500 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x500 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x500 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x500 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x500 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x500 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to visualize continuous features\n",
    "def visualize_continuous_features(df, target_variable):\n",
    "    continuous_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    for column in continuous_columns:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.boxplot(x=target_variable, y=column, data=df)\n",
    "        plt.title(f'Relationship between {column} and {target_variable}')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{column}.png\", bbox_inches='tight')\n",
    "        plt.clf()\n",
    "\n",
    "# Visualize continuous features\n",
    "visualize_continuous_features(readdata, target_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate features from target variable and encode y as 0 or 1\n",
    "\n",
    "df = readdata.copy().drop(columns='vital_status', axis=1)\n",
    "y = np.array(1*pd.get_dummies(readdata.copy()['vital_status'], drop_first=True)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The categorical columns are: ['primary_diagnosis', 'prior_malignancy', 'prior_treatment', 'ethnicity', 'race'] and the length is 5.\n",
      "The continuous columns are: ['ajcc_pathologic_stage', 'year_of_diagnosis', 'pharm_treatment', 'radiation', 'age_at_index', 'year_of_birth'] and the length is 6.\n",
      "The boolean columns are: [] and the length is 0.\n"
     ]
    }
   ],
   "source": [
    "# list categorical, continuous and boolean columns\n",
    "\n",
    "categorical_columns = [col for col in df.columns if df[col].dtype == \"object\"]\n",
    "print(f\"The categorical columns are: {categorical_columns} and the length is {len(categorical_columns)}.\")\n",
    "\n",
    "continuous_columns = [cols for cols in df.columns if df[cols].dtype in [\"float64\", \"int64\"]]\n",
    "print(f\"The continuous columns are: {continuous_columns} and the length is {len(continuous_columns)}.\")\n",
    "\n",
    "boolean_columns = [col_ for col_ in df.columns if df[col_].dtype == \"bool\"]\n",
    "print(f\"The boolean columns are: {boolean_columns} and the length is {len(boolean_columns)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode categorical columns\n",
    "\n",
    "df2 = df[categorical_columns].copy()\n",
    "# Iterate through each column in the dataframe\n",
    "for column in categorical_columns:\n",
    "    one_hot = pd.get_dummies(df2[column], prefix=column).astype(int)\n",
    "    # Drop the original column from the dataframe\n",
    "    df2.drop(column, axis=1, inplace=True)\n",
    "    # Join the new one-hot encoded columns to the original dataframe\n",
    "    df2 = df2.join(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# concatenate the continuous dataframe and one-hot encoded dataframe\n",
    "new_df = pd.concat([df[continuous_columns], df2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the one hot encoded data\n",
    "new_df.to_csv(\"one_hot_encoded_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Training and Validation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "\n",
    "# first split data into train and test\n",
    "xtrain_, xtest, ytrain_, ytest = train_test_split(new_df, y, train_size=0.8, shuffle=True, stratify=y, random_state=123)\n",
    "\n",
    "# scale data\n",
    "scale = MinMaxScaler()\n",
    "xtrain_ = scale.fit_transform(xtrain_)\n",
    "xtest = scale.transform(xtest)\n",
    "\n",
    "# perform stratified k-fold to split xtrain_ into xtrain_train and validation sets\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "basemodel = DecisionTreeClassifier(random_state=123)\n",
    "best_model = None\n",
    "best_val_score = -np.inf\n",
    "\n",
    "base_scores = []\n",
    "model_scores = []\n",
    "model_precision = []\n",
    "model_recall = []\n",
    "model_selectivity = []\n",
    "model_fp = [] \n",
    "model_fn = []\n",
    "\n",
    "# Perform stratified k-fold split with nested validation set\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(xtrain_, ytrain_)):\n",
    "    xtrain_train, ytrain_train = xtrain_[train_idx], ytrain_[train_idx]\n",
    "    xval, yval = xtrain_[val_idx], ytrain_[val_idx]\n",
    "\n",
    "    # train base model\n",
    "    basemodel.fit(xtrain_train, ytrain_train)\n",
    "    base_val_pred = basemodel.predict(xval)\n",
    "    base_val_score = accuracy_score(yval, base_val_pred)\n",
    "    \n",
    "    TN = confusion_matrix(yval, base_val_pred)[0,0]\n",
    "    FP = confusion_matrix(yval, base_val_pred)[0,1]\n",
    "    FN = confusion_matrix(yval, base_val_pred)[1,0]\n",
    "    TP = confusion_matrix(yval, base_val_pred)[1,1]\n",
    "        \n",
    "    precision = np.round(TP/(TP + FP),4)\n",
    "    recall = np.round(TP/(TP+FN),4)\n",
    "    selectivity = np.round(TN/(TN+FP),4)\n",
    "    fp = np.round(FP/(FP+TN),4)\n",
    "    fn = np.round(FN/(TP+FN),4)\n",
    "    \n",
    "    base_scores.append([base_val_score,precision,recall,selectivity,fp,fn])\n",
    "\n",
    "    # Train multiple models\n",
    "    models = {'rf': RandomForestClassifier(random_state=123),\n",
    "              'adaboost': AdaBoostClassifier(algorithm='SAMME'),\n",
    "              'svc': SVC(),\n",
    "              'logreg': LogisticRegression(max_iter=1000),\n",
    "              'knn': KNeighborsClassifier(n_neighbors=10),\n",
    "              'mlp': MLPClassifier(max_iter=10000)\n",
    "              }\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        # Train the model\n",
    "        model.fit(xtrain_train, ytrain_train)\n",
    "        # Validate the model\n",
    "        y_val_predict = model.predict(xval)\n",
    "        val_score = accuracy_score(yval, y_val_predict)\n",
    "\n",
    "        #print(f\"Fold {fold + 1}, Model: {model_name}, Validation Accuracy: {val_score:.4f}\")\n",
    "\n",
    "        model_scores.append((model_name, val_score))\n",
    "        \n",
    "        ## Other classification metrics\n",
    "        TN = confusion_matrix(yval, y_val_predict)[0,0]\n",
    "        FP = confusion_matrix(yval, y_val_predict)[0,1]\n",
    "        FN = confusion_matrix(yval, y_val_predict)[1,0]\n",
    "        TP = confusion_matrix(yval, y_val_predict)[1,1]\n",
    "        \n",
    "        precision = np.round(TP/(TP + FP),4)\n",
    "        recall = np.round(TP/(TP+FN),4)\n",
    "        selectivity = np.round(TN/(TN+FP),4)\n",
    "        fp = np.round(FP/(FP+TN),4)\n",
    "        fn = np.round(FN/(TP+FN),4)    \n",
    "        \n",
    "        model_precision.append((model_name,precision))\n",
    "        model_recall.append((model_name,recall))\n",
    "        model_selectivity.append((model_name,selectivity))\n",
    "        model_fp.append((model_name,fp))\n",
    "        model_fn.append((model_name,fn))\n",
    "        \n",
    "        #print(f\"{model_name}, Fold {fold+1}, \")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Obtain the best model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Accuracy of the Base Model: 0.8586\n",
      "Best Model: mlp with Average Validation Accuracy: 0.9065\n"
     ]
    }
   ],
   "source": [
    "# Print average accuracy of the base model\n",
    "\n",
    "base_averages = [item[0] for item in base_scores]\n",
    "average_base_score = np.mean(base_averages)\n",
    "print(f\"Average Validation Accuracy of the Base Model: {average_base_score:.4f}\")\n",
    "\n",
    "\n",
    "# Calculate average validation accuracy for each model and find the best model\n",
    "average_model_scores = {}\n",
    "list_scores = []\n",
    "for model_name, model in models.items():\n",
    "    for name, score in model_scores:\n",
    "        if name == model_name:\n",
    "            list_scores.append(score)\n",
    "    average = np.mean(list_scores)\n",
    "    average_model_scores[model_name] = average\n",
    "    list_scores = []\n",
    "    \n",
    "best_model_name = max(average_model_scores, key=average_model_scores.get)\n",
    "best_model_avg_score = average_model_scores[best_model_name]\n",
    "\n",
    "print(f\"Best Model: {best_model_name} with Average Validation Accuracy: {best_model_avg_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation precision of the Base Model: 0.4692\n",
      "Best Model: knn with Average Validation Precision: 0.9314\n"
     ]
    }
   ],
   "source": [
    "# Print average precision of the base model\n",
    "base_precision = [item[1] for item in base_scores]\n",
    "average_base_precision = np.mean(base_precision)\n",
    "print(f\"Average Validation precision of the Base Model: {average_base_precision:.4f}\")\n",
    "\n",
    "# Calculate average validation precision for each model and find the best model\n",
    "average_model_scores = {}\n",
    "list_scores = []\n",
    "for model_name, model in models.items():\n",
    "    for name, score in model_precision:\n",
    "        if name == model_name:\n",
    "            list_scores.append(score)\n",
    "    average = np.mean(list_scores)\n",
    "    average_model_scores[model_name] = average\n",
    "    list_scores = []\n",
    "    \n",
    "best_model_name = max(average_model_scores, key=average_model_scores.get)\n",
    "best_model_avg_precision = average_model_scores[best_model_name]\n",
    "\n",
    "print(f\"Best Model: {best_model_name} with Average Validation Precision: {best_model_avg_precision:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation recall of the Base Model: 0.4952\n",
      "Best Model: mlp with Average Validation Recall: 0.4857\n"
     ]
    }
   ],
   "source": [
    "# Print average recall of the base model\n",
    "base_recall = [item[2] for item in base_scores]\n",
    "average_base_recall = np.mean(base_recall)\n",
    "print(f\"Average Validation recall of the Base Model: {average_base_recall:.4f}\")\n",
    "\n",
    "# Calculate average validation recall for each model and find the best model\n",
    "average_model_scores = {}\n",
    "list_scores = []\n",
    "for model_name, model in models.items():\n",
    "    for name, score in model_recall:\n",
    "        if name == model_name:\n",
    "            list_scores.append(score)\n",
    "    average = np.mean(list_scores)\n",
    "    average_model_scores[model_name] = average\n",
    "    list_scores = []\n",
    "    \n",
    "best_model_name = max(average_model_scores, key=average_model_scores.get)\n",
    "best_model_avg_recall = average_model_scores[best_model_name]\n",
    "\n",
    "print(f\"Best Model: {best_model_name} with Average Validation Recall: {best_model_avg_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation selectivity of the Base Model: 0.9141\n",
      "Best Model: knn with Average Validation selectivity: 0.9971\n"
     ]
    }
   ],
   "source": [
    "# Print average selectivity of the base model\n",
    "base_selectivity = [item[3] for item in base_scores]\n",
    "average_base_selectivity = np.mean(base_selectivity)\n",
    "print(f\"Average Validation selectivity of the Base Model: {average_base_selectivity:.4f}\")\n",
    "\n",
    "# Calculate average validation selectivity for each model and find the best model\n",
    "average_model_scores = {}\n",
    "list_scores = []\n",
    "for model_name, model in models.items():\n",
    "    for name, score in model_selectivity:\n",
    "        if name == model_name:\n",
    "            list_scores.append(score)\n",
    "    average = np.mean(list_scores)\n",
    "    average_model_scores[model_name] = average\n",
    "    list_scores = []\n",
    "    \n",
    "best_model_name = max(average_model_scores, key=average_model_scores.get)\n",
    "best_model_avg_selectivity = average_model_scores[best_model_name]\n",
    "\n",
    "print(f\"Best Model: {best_model_name} with Average Validation selectivity: {best_model_avg_selectivity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation false positivity of the Base Model: 0.0859\n",
      "Best Model: knn with Average Validation false positivity: 0.0029\n"
     ]
    }
   ],
   "source": [
    "# Print average false positivity of the base model\n",
    "base_fp = [item[4] for item in base_scores]\n",
    "average_base_fp = np.mean(base_fp)\n",
    "print(f\"Average Validation false positivity of the Base Model: {average_base_fp:.4f}\")\n",
    "\n",
    "# Calculate average validation fp for each model and find the best model\n",
    "average_model_scores = {}\n",
    "list_scores = []\n",
    "for model_name, model in models.items():\n",
    "    for name, score in model_fp:\n",
    "        if name == model_name:\n",
    "            list_scores.append(score)\n",
    "    average = np.mean(list_scores)\n",
    "    average_model_scores[model_name] = average\n",
    "    list_scores = []\n",
    "    \n",
    "best_model_name = min(average_model_scores, key=average_model_scores.get)\n",
    "best_model_avg_fp = average_model_scores[best_model_name]\n",
    "\n",
    "print(f\"Best Model: {best_model_name} with Average Validation false positivity: {best_model_avg_fp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation false negativity of the Base Model: 0.5048\n",
      "Best Model: mlp with Average Validation false negativity: 0.5143\n"
     ]
    }
   ],
   "source": [
    "# Print average false positivity of the base model\n",
    "base_fn = [item[5] for item in base_scores]\n",
    "average_base_fn = np.mean(base_fn)\n",
    "print(f\"Average Validation false negativity of the Base Model: {average_base_fn:.4f}\")\n",
    "\n",
    "# Calculate average validation fn for each model and find the best model\n",
    "average_model_scores = {}\n",
    "list_scores = []\n",
    "for model_name, model in models.items():\n",
    "    for name, score in model_fn:\n",
    "        if name == model_name:\n",
    "            list_scores.append(score)\n",
    "    average = np.mean(list_scores)\n",
    "    average_model_scores[model_name] = average\n",
    "    list_scores = []\n",
    "    \n",
    "best_model_name = min(average_model_scores, key=average_model_scores.get)\n",
    "best_model_avg_fn = average_model_scores[best_model_name]\n",
    "\n",
    "print(f\"Best Model: {best_model_name} with Average Validation false negativity: {best_model_avg_fn:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prediction on Test Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Hyperparameters: {'max_depth': 10, 'n_estimators': 200}\n",
      "Test Accuracy of the Best Tuned Model: 0.9246\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "        'n_estimators': [10, 50, 100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30]\n",
    "    }\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=123), param_grid, cv=skf, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search\n",
    "grid_search.fit(xtrain_, ytrain_)\n",
    "best_tuned_model = grid_search.best_estimator_\n",
    "print(f\"Best Model Hyperparameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Test the best tuned model on the test set\n",
    "test_predictions = best_tuned_model.predict(xtest)\n",
    "test_score = accuracy_score(ytest, test_predictions)\n",
    "test_precision = precision_score(ytest, test_predictions)\n",
    "test_recall = recall_score(ytest, test_predictions)\n",
    "print(f\"Test Accuracy of the Best Tuned Model: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
